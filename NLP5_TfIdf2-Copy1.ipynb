{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aebf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"temp_data3.csv\")\n",
    "df=df.iloc[:4000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "list1=df.Sentiment.values\n",
    "for i in range(len(list1)):\n",
    "    if(list1[i]==\"mixed\"):\n",
    "        df.drop([i],axis=0,inplace=True)\n",
    "    elif(list1[i]==\"neutral\"):\n",
    "        df.drop([i],axis=0,inplace=True)\n",
    "\n",
    "df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b68a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df[\"cleaned_text\"]\n",
    "sentiment=df[\"Sentiment\"]\n",
    "dataf=pd.concat([text,sentiment],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=dataf[\"cleaned_text\"].apply(lambda x: \" \".join(x.lower() for x in x.split() if x.isalpha()))\n",
    "data=data.str.replace(\"[^\\w\\s]\",\"\")#harf dışı simgeler,noktalama\n",
    "data= data.str.replace(\" \\d+\", \" \")#sayılar \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "duraklama_kelimeleri = stopwords.words()\n",
    "data=pd.DataFrame(data,columns=[\"cleaned_text\"])\n",
    "data=data[\"cleaned_text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in duraklama_kelimeleri))\n",
    "data=pd.concat([data,sentiment],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb400b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = data['cleaned_text'].values\n",
    "labels = data['Sentiment'].values\n",
    "cleaned_text=list(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1#kelimenin kaç dökümanda olduğu\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():#cümle kelimeleri\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}#kelime indexleme\n",
    "    Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "Vocabulary, idf_of_vocabulary=fit(cleaned_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocabulary,idf_values):\n",
    "    tfidf=np.zeros(len(vocabulary))\n",
    "    number_of_words_in_sentence=Counter(dataset.split())#yorumdaki kelimelerin sayısı\n",
    "    for word in dataset.split():\n",
    "        if word in  list(vocabulary.keys()):#(kelimenin yorumdaki sayısı/yorumun uzunluğu)*kelimenin idf değeri\n",
    "            tf_idf_value=(number_of_words_in_sentence[word]/len(dataset.split()))*(idf_values[word])\n",
    "            tfidf[vocabulary[word]]=tf_idf_value\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd01893",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablo = np.zeros((len(cleaned_text),len(Vocabulary)))\n",
    "tablo =pd.DataFrame(tablo,columns=Vocabulary.keys())\n",
    "tfidf=np.zeros(len(Vocabulary))\n",
    "for i in range(len(cleaned_text)):\n",
    "    tfidf=transform(cleaned_text[i],Vocabulary,idf_of_vocabulary)\n",
    "    tablo.loc[i] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ce1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablo=normalize(tablo, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "tablo =pd.DataFrame(tablo,columns=Vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b7c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(tablo, encoded_labels, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):    \n",
    "    output = 1 / (1 + np.exp(-input))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685995bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    loss = -np.mean(y*(np.log(y_hat)) + (1-y)*np.log(1-y_hat))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    dw = (1/m)*np.dot(X.T, (y_hat - y))\n",
    "    \n",
    "    db = (1/m)*np.sum((y_hat - y)) \n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b84bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, bs, epochs, lr):\n",
    "    \n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    \n",
    "    y = y.reshape(m,1)\n",
    "\n",
    "    losses=[]\n",
    "    losses2=[]\n",
    "    accuracy_tr=[]\n",
    "    accuracy_te=[]\n",
    "    iterations2=[]\n",
    "    weight_dict = {}\n",
    "    bias_dict={}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        weight_dict[epoch] = defaultdict(lambda: 0)\n",
    "        bias_dict[epoch] = defaultdict(lambda: 0)\n",
    "        for i in range((m-1)//bs + 1):\n",
    "           \n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            \n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "            weight_dict[epoch][i]=w\n",
    "            bias_dict[epoch][i]=b\n",
    "\n",
    "        l = loss(y, sigmoid(np.dot(X, w) + b))\n",
    "        losses.append(l)\n",
    "        pred=predict(X,w,b)\n",
    "        accuracy_tr.append(accuracy_score(y,pred))\n",
    "        \n",
    "        l2 = loss(test_labels, sigmoid(np.dot(test_sentences, w) + b))\n",
    "        losses2.append(l2)\n",
    "        pred2=predict(test_sentences,w,b)\n",
    "        accuracy_te.append(accuracy_score(test_labels,pred2))\n",
    "        iterations2.append(epoch)\n",
    "        \n",
    "    return w, b, dw, db, losses,losses2,accuracy_tr,accuracy_te,iterations2,weight_dict,bias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a48530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w,b):\n",
    "    \n",
    "\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    pred_class = []\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in preds]\n",
    "    \n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb164c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, dw, db,losses,losses2,accuracy_tr,accuracy_te,iterations2,weight_dict,bias_dict = train(train_sentences, train_labels, bs=100, epochs=300, lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b , db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05657e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape,dw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15969af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c311dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations2,accuracy_tr)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'accuracy_tr' )  \n",
    "plt.title(\"0.02-300-TfIdf-Acc_Tr\")\n",
    "#plt.savefig(\"0.02-500-TfIdf-Acc_Tr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ef53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations2,accuracy_te)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'accuracy_te' )  \n",
    "plt.title(\"0.02-300-TfIdf-Acc_Te\")\n",
    "#plt.savefig(\"0.02-500-TfIdf-Acc_Tr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations2,losses)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'losses' )  \n",
    "plt.title(\"0.02-300-TfIdf-Loss-Tr\")\n",
    "#plt.savefig(\"0.02-500-TfIdf-Acc_Tr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9babd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations2,losses2)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'losses2' )  \n",
    "plt.title(\"0.02-300-TfIdf-Loss-Te\")\n",
    "#plt.savefig(\"0.02-500-TfIdf-Acc_Tr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=predict(test_sentences,w,b)\n",
    "accuracy_score(test_labels,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class_names=[\"0\",\"1\"]\n",
    "\n",
    "print(classification_report(test_labels, pred, target_names=class_names))\n",
    "\n",
    "print(confusion_matrix(test_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643db1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
