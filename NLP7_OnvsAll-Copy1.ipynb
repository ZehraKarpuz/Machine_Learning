{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8598e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"temp_data3.csv\")\n",
    "df=df.iloc[:4000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[\"cleaned_text\"].apply(lambda x: \" \".join(x.lower() for x in x.split() if x.isalpha()))\n",
    "data=data.str.replace(\"[^\\w\\s]\",\"\")#harf dışı simgeler,noktalama\n",
    "data= data.str.replace(\" \\d+\", \" \")#sayılar \n",
    "nltk.download(\"stopwords\")\n",
    "duraklama_kelimeleri = stopwords.words()\n",
    "data=pd.DataFrame(data,columns=[\"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513793a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text=list(data[\"cleaned_text\"])\n",
    "name=[\"good\",\"love\",\"some\"]\n",
    "data_label=[]\n",
    "sayac=0\n",
    "for sentences in cleaned_text:\n",
    "    for word in sentences.split():\n",
    "        if word not in name:\n",
    "            sayac=3\n",
    "        elif(word==name[0]):\n",
    "            sayac=0\n",
    "            break\n",
    "        elif(word==name[1]):\n",
    "            sayac=1\n",
    "            break\n",
    "        elif(word==name[2]):\n",
    "            sayac=2\n",
    "            break\n",
    "    if(sayac==0):\n",
    "        data_label.append(0)\n",
    "    elif(sayac==1):\n",
    "        data_label.append(1)\n",
    "    elif(sayac==2):\n",
    "        data_label.append(2)\n",
    "    elif(sayac==3):\n",
    "        data_label.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce575522",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label=pd.DataFrame(data_label,columns=[\"Labels\"])\n",
    "dataf=pd.concat([data,data_label],axis=1)\n",
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "list1=dataf.Labels.values\n",
    "for i in range(len(list1)):\n",
    "    if(list1[i]==3):\n",
    "        dataf.drop([i],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2950b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text=list(dataf[\"cleaned_text\"])\n",
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f31b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(dataf[\"Labels\"]==0),np.sum(dataf[\"Labels\"]==1),np.sum(dataf[\"Labels\"]==2),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c70a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1#kelimenin kaç dökümanda olduğu\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():#cümle kelimeleri\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}#kelime indexleme\n",
    "    Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words\n",
    "Vocabulary, idf_of_vocabulary=fit(cleaned_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f04a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocabulary,idf_values):\n",
    "    tfidf=np.zeros(len(vocabulary))\n",
    "    number_of_words_in_sentence=Counter(dataset.split())#yorumdaki kelimelerin sayısı\n",
    "    for word in dataset.split():\n",
    "        if word in  list(vocabulary.keys()):#(kelimenin yorumdaki sayısı/yorumun uzunluğu)*kelimenin idf değeri\n",
    "            tf_idf_value=(number_of_words_in_sentence[word]/len(dataset.split()))*(idf_values[word])\n",
    "            tfidf[vocabulary[word]]=tf_idf_value\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d224277",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablo = np.zeros((len(cleaned_text),len(Vocabulary)))\n",
    "tablo =pd.DataFrame(tablo,columns=Vocabulary.keys())\n",
    "tfidf=np.zeros(len(Vocabulary))\n",
    "for i in range(len(cleaned_text)):\n",
    "    tfidf=transform(cleaned_text[i],Vocabulary,idf_of_vocabulary)\n",
    "    tablo.loc[i] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c78c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablo=normalize(tablo, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "tablo =pd.DataFrame(tablo,columns=Vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dataf[\"Labels\"]\n",
    "y1 = np.zeros([tablo.shape[0], len(labels.unique())])\n",
    "y1 = pd.DataFrame(y1)\n",
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d17f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(labels.unique())):\n",
    "    for j in range(0, len(y1)):\n",
    "        if labels[j] == labels.unique()[i]:\n",
    "            y1.iloc[j, i] = 1#yorum hangi etikete sahipse 1 diğerleri 0\n",
    "        else: \n",
    "            y1.iloc[j, i] = 0\n",
    "y1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=y1.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(tablo,y1, stratify = y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=test_labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "orj=np.array([]) \n",
    "for i in range(len(test_labels)):\n",
    "    for j in range(len(labels.unique())):\n",
    "        if(test_labels[j][i]==1):\n",
    "            orj=np.append(orj,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eae42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5130256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):    \n",
    "    output = 1 / (1 + np.exp(-input))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2deeafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_parameters = {} \n",
    "init_parameters[\"weight\"] = np.random.rand(train_sentences.shape[1])\n",
    "init_parameters[\"bias\"] = 0\n",
    "def optimize(x, y,learning_rate,bs,iterations,parameters): \n",
    "    size = x.shape[0]#train sayısı\n",
    "    weight = parameters[\"weight\"] \n",
    "    bias = parameters[\"bias\"]\n",
    "    weight_dict = {}\n",
    "    dw=[]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        weight_dict[i] = defaultdict(lambda: 0)\n",
    "        \n",
    "        for j in range((size-1)//bs+1):#10\n",
    "            start_i = j*100\n",
    "            end_i = start_i + 100\n",
    "            xb = x[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            size2=xb.shape[0]\n",
    "            sigma = sigmoid(np.dot(xb, weight) + bias)\n",
    "            dW = (1/size2) * np.dot(xb.T, (sigma - yb))\n",
    "            db = (1/size2) * np.sum((sigma - yb))\n",
    "            weight -= learning_rate * dW\n",
    "            bias -= learning_rate * db \n",
    "            \n",
    "            parameters[\"weight\"] = weight\n",
    "            parameters[\"bias\"] = bias\n",
    "\n",
    "            weight_dict[i][j]=weight\n",
    "            dw.append(dW)\n",
    "\n",
    "    return parameters,weight_dict,dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, learning_rate,bs,iterations):\n",
    "    parameters_out,weight_dict,dw = optimize(x, y, learning_rate, bs, iterations ,init_parameters)\n",
    "    return parameters_out,weight_dict,dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04450aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_out=[]\n",
    "for i in range(len(labels.unique())):\n",
    "    train_labels1=np.array(train_labels[i])\n",
    "    parameters_out1,weight_dict,dw= train(train_sentences, train_labels1,learning_rate = 0.02,bs=100,iterations = 200)\n",
    "    parameters_out.append(parameters_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw # her 10 batch'ten sonra yeni epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488afcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_values_te=[]\n",
    "for i in range(len(labels.unique())):\n",
    "    output_values1 = np.dot(test_sentences, parameters_out[i][\"weight\"]) + parameters_out[i][\"bias\"]\n",
    "    output_values_te.append(output_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3787fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cfd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_values_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=np.array([]) \n",
    "for i in range(len(test_sentences)):\n",
    "    for j in range(len(labels.unique())):\n",
    "        list1=np.array([]) \n",
    "        list1=np.append(list1,output_values_te[j][i])\n",
    "    pred=np.append(pred,np.argmax(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b04e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(orj,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class_names=[\"0\",\"1\",\"2\"]\n",
    "\n",
    "print(classification_report(orj, pred, target_names=class_names))\n",
    "\n",
    "print(confusion_matrix(orj, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62712bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
