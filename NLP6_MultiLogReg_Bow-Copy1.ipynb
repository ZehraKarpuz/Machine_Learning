{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247affa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"temp_data3.csv\")\n",
    "df=df.iloc[:4000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c289f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[\"cleaned_text\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "data=data.str.replace(\"[^\\w\\s]\",\"\")#harf dışı simgeler,noktalama\n",
    "data= data.str.replace(\" \\d+\", \" \")#sayılar \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "duraklama_kelimeleri = stopwords.words()\n",
    "data=pd.DataFrame(data,columns=[\"cleaned_text\"])\n",
    "data=data[\"cleaned_text\"].apply(lambda x: \" \".join(x for x in x.split() if x not in duraklama_kelimeleri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data,columns=[\"cleaned_text\"])\n",
    "cleaned_text = data['cleaned_text'].values\n",
    "cleaned_text=np.array(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[\"good\",\"bad\",\"negative\"]\n",
    "data_label=[]\n",
    "sayac=0\n",
    "for sentences in cleaned_text:\n",
    "    for word in sentences.split():\n",
    "        if word not in name:\n",
    "            sayac=0\n",
    "        elif(word==name[0]):\n",
    "            sayac=1\n",
    "            break\n",
    "        elif(word==name[1]):\n",
    "            sayac=2\n",
    "            break\n",
    "        elif(word==name[2]):\n",
    "            sayac=3\n",
    "            break\n",
    "    if(sayac==0):\n",
    "        data_label.append(0)\n",
    "    elif(sayac==1):\n",
    "        data_label.append(1)\n",
    "    elif(sayac==2):\n",
    "        data_label.append(2)\n",
    "    elif(sayac==3):\n",
    "        data_label.append(3)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label=pd.DataFrame(data_label,columns=[\"Labels\"])\n",
    "dataf=pd.concat([data,data_label],axis=1)\n",
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0025f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "#clean the corpus.\n",
    "sentences = []\n",
    "vocab = []\n",
    "for sent in cleaned_text:\n",
    "    x = word_tokenize(sent)\n",
    "    sentence = [w.lower() for w in x if w.isalpha() ]\n",
    "    sentences.append(sentence)\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "len_vector = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ceecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {}\n",
    "i = 0\n",
    "for word in vocab:\n",
    "    index_word[word] = i \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = defaultdict(int)\n",
    "vec = np.zeros(len_vector)\n",
    "for item in sentences[1]:\n",
    "    count_dict[item] += 1\n",
    "for key,item in count_dict.items():\n",
    "    vec[index_word[key]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sent):\n",
    "    count_dict = defaultdict(int)\n",
    "    vec = np.zeros(len_vector)\n",
    "    for item in sent:\n",
    "        count_dict[item] += 1\n",
    "    for key,item in count_dict.items():\n",
    "        vec[index_word[key]] = item\n",
    "    return vec   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17467550",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablo = np.zeros((len(sentences),len(vocab)))\n",
    "tablo =pd.DataFrame(tablo,columns=vocab)\n",
    "i=0\n",
    "j=0\n",
    "for i in range(len(vocab)):\n",
    "    if i<len(sentences):\n",
    "        vector = bag_of_words(sentences[j])\n",
    "        tablo.loc[i] = vector\n",
    "        j+=1\n",
    "vector = vector.astype(\"float64\")\n",
    "tablo = tablo.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=dataf[\"Labels\"].values\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(tablo, labels, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ae0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.rand(4, train_sentences.shape[1])\n",
    "biases = np.random.rand(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca787f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearPredict(featureMat, weights, biases):\n",
    " \n",
    "    logitScores = np.array([np.empty([4]) for i in range(featureMat.shape[0])]) # her yorumun 4 etiket için olasılığı\n",
    "    \n",
    "    for i in range(featureMat.shape[0]): \n",
    "        logitScores[i] = (weights.dot(featureMat[i].reshape(-1,1)) + biases).reshape(-1) \n",
    "    \n",
    "    return logitScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130540c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_sentences.to_numpy()\n",
    "logitTest = linearPredict(features, weights,biases )\n",
    "logitTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxNormalizer(logitMatrix):\n",
    "  \n",
    "    probabilities = np.array([np.empty([4]) for i in range(logitMatrix.shape[0])]) #(veri uzunluğu kadar,4)\n",
    "    for i in range(logitMatrix.shape[0]):\n",
    "        exp = np.exp(logitMatrix[i]) \n",
    "        sumOfArr = np.sum(exp) #4 adet üslü yazılan olasılıkların değerlerini toplar\n",
    "        probabilities[i] = exp/sumOfArr #4 adet üslü yazılan olasılıklar toplamlarına bölünür\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e785495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomialLogReg(features, weights, biases):\n",
    "    \n",
    "    logitScores = linearPredict(features, weights, biases)#her yorumun 4 etiket için olasılıkları tutulur\n",
    "    probabilities = softmaxNormalizer(logitScores)\n",
    "    predictions = np.array([np.argmax(i) for i in probabilities]) #olasılığı en yüksek olan ile etiketlendirilir\n",
    "    return probabilities, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a90b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities, predictions = multinomialLogReg(features, weights, biases)\n",
    "print(probabilities.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(probabilities, target):\n",
    "    \n",
    "    n_samples = probabilities.shape[0]\n",
    "    CELoss = 0\n",
    "    for sample, i in zip(probabilities, target):\n",
    "        CELoss += -np.log(sample[i])#gerçek etiketlerin olasılıkları toplanır\n",
    "    CELoss /= n_samples#ort değer için veri sayısına bölünür\n",
    "    return CELoss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a964c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochGradDes(learning_rate, epochs, target, features, weights, biases):\n",
    "\n",
    "    weights = np.random.rand(4, train_sentences.shape[1])\n",
    "    biases = np.random.rand(4, 1)\n",
    "    target = target.astype(int)\n",
    "    loss_list = np.array([]) \n",
    "    loss_list2 = np.array([])\n",
    "    acc_test=np.array([])\n",
    "    acc_train=np.array([])\n",
    "    iterations=np.array([])\n",
    "    size=features.shape[0]\n",
    "    for i in range(epochs):\n",
    "        for j in range((size-1)//100+1):\n",
    "            start_i = j*100\n",
    "            end_i = start_i + 100\n",
    "            xb = features[start_i:end_i]\n",
    "            yb = target[start_i:end_i]\n",
    "            probabilities, trainPredictions = multinomialLogReg(xb, weights, biases) #yorumların 4 etiket için olasılıkları hesaplanır\n",
    "            probabilities[np.arange(xb.shape[0]),yb] -= 1 # 3000 yorumun doğru etiketinin olasılığı-1\n",
    "            grad_weight = probabilities.T.dot(xb) \n",
    "            grad_biases = np.sum(probabilities, axis = 0).reshape(-1,1) \n",
    "            weights -= (learning_rate * grad_weight)\n",
    "            biases -= (learning_rate * grad_biases)\n",
    "            \n",
    "        probabilities, trainPredictions = multinomialLogReg(features, weights, biases) \n",
    "        acc_train=np.append(acc_train,accuracy_score(target,trainPredictions))\n",
    "        Loss = crossEntropyLoss(probabilities, target) \n",
    "        loss_list = np.append(loss_list, Loss) \n",
    "        \n",
    "        \n",
    "        testProbabilities, testPredictions = multinomialLogReg(test_sentences.to_numpy(), weights, biases)\n",
    "        acc_test=np.append(acc_test,accuracy_score(test_labels,testPredictions))\n",
    "        Loss2 = crossEntropyLoss(testProbabilities, test_labels) \n",
    "        loss_list2 = np.append(loss_list2, Loss2) \n",
    "        iterations=np.append(iterations,i)\n",
    "        \n",
    "    return weights, biases, loss_list,loss_list2,acc_test,acc_train,iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedWeights, updatedBiases, loss_list,loss_list2,acc_test,acc_train,iterations = stochGradDes(0.02, 100, train_labels, train_sentences.to_numpy(), weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations,acc_train)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'accuracy_tr' )  \n",
    "plt.title(\"0.02-100-Bow-AccTrain_Multi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea94a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations,acc_test)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'accuracy_te' )  \n",
    "plt.title(\"0.02-100-TfIdf-AccTest_Multi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations,loss_list)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'loss_train' )  \n",
    "plt.title(\"0.02-100-TfIdf-LossTrain_Multi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50711309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iterations,loss_list2)  \n",
    "plt.xlabel( 'iterations' ) \n",
    "plt.ylabel( 'loss_test' )  \n",
    "plt.title(\"0.02-100-TfIdf-LossTest_Multi\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbc9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
